# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vK6JhO1PXK31MMx4ba7KVzgT7YQbpI3P
"""

from transformers import GPT2ForSequenceClassification, GPT2Config

# Ensure the configuration matches the pre-trained model
config = GPT2Config.from_pretrained('gpt2', num_labels=your_number_of_labels)
model = GPT2ForSequenceClassification(config)

try:
    # Load the pre-trained model weights
    model.load_state_dict(torch.load('path_to_your_model_weights.pth'), strict=False)
except RuntimeError as e:
    print("There was a mismatch between the model architecture and the pre-trained weights:")
    print(e)

# Check for any missing or unexpected keys
missing_keys, unexpected_keys = model.load_state_dict(torch.load('path_to_your_model_weights.pth'), strict=False)
print(f"Missing keys: {missing_keys}")
print(f"Unexpected keys: {unexpected_keys}")

# If there are layers that have been customized, you may need to initialize their weights manually
# For example, if you've added a custom head to GPT2, initialize its weights here.

# After handling the missing and unexpected keys, you can now proceed with training or inference
# ...

# Note: Replace 'path_to_your_model_weights.pth' with the actual path to your model weights.
# Replace 'your_number_of_labels' with the actual number of labels for your sequence classification task.