{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMWrKcP9f2PXdFYjMBpLRth"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB_PHjgnymMu"
      },
      "outputs": [],
      "source": [
        "!pip install pycryptodome\n",
        "!pip -q install langchain\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install datasets loralib sentencepiece\n",
        "!pip -q install pypdf\n",
        "!pip install torch\n",
        "!pip -q install sentence_transformers\n",
        "!pip install accelerate>=0.21.0\n",
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install pycryptodome\n",
        "!pip install gdown\n",
        "!pip install nltk\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "import torch\n",
        "import requests\n",
        "from Crypto.PublicKey import RSA\n",
        "from Crypto.Cipher import PKCS1_OAEP\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "import torch\n",
        "import requests\n",
        "import nltk\n",
        "import random\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "import torch\n",
        "from Crypto.PublicKey import RSA\n",
        "from Crypto.Cipher import PKCS1_OAEP"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "training :"
      ],
      "metadata": {
        "id": "YBOarbw8pxt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Function to load dataset from file\n",
        "def load_dataset_from_file(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "# Google Drive file IDs\n",
        "scenario1_file_id = \"\"**\"\"\n",
        "scenario2_file_id = \"**\"\n",
        "\n",
        "# Download files from Google Drive using file IDs\n",
        "scenario1_file_url = f\"https://drive.google.com/uc?id={scenario1_file_id}\"\n",
        "scenario2_file_url = f\"https://drive.google.com/uc?id={scenario2_file_id}\"\n",
        "\n",
        "scenario1_file_path = \"/content/scenario1.csv\"\n",
        "scenario2_file_path = \"/content/scenario2.csv\"\n",
        "\n",
        "gdown.download(scenario1_file_url, scenario1_file_path, quiet=False)\n",
        "gdown.download(scenario2_file_url, scenario2_file_path, quiet=False)\n",
        "\n",
        "# Load datasets from local files\n",
        "scenario1_df = load_dataset_from_file(scenario1_file_path)\n",
        "scenario2_df = load_dataset_from_file(scenario2_file_path)\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the tokenizer with the specified pad token ID\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token=str(50257))\n",
        "\n",
        "# Tokenize the input text for scenario 1\n",
        "train_encodings_scenario1 = tokenizer(scenario1_df['Name1'].tolist(), scenario1_df['Name2'].tolist(), truncation=True, padding=True)\n",
        "train_labels_scenario1 = torch.tensor(scenario1_df['DLabel'].tolist())\n",
        "# Create PyTorch dataset for scenario 1\n",
        "train_dataset_scenario1 = TensorDataset(torch.tensor(train_encodings_scenario1['input_ids']),\n",
        "                                        torch.tensor(train_encodings_scenario1['attention_mask']),\n",
        "                                        train_labels_scenario1)\n",
        "# Define data loader for scenario 1\n",
        "batch_size = 16  # adjust batch size as needed\n",
        "train_dataloader_scenario1 = DataLoader(train_dataset_scenario1, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Tokenize the input text for scenario 2\n",
        "\n",
        "input_text_scenario2 = scenario2_df['PName'] + \" \" + scenario2_df['Name']\n",
        "train_encodings_scenario2 = tokenizer(input_text_scenario2.tolist(), truncation=True, padding=True)\n",
        "train_labels_scenario2 = torch.tensor(scenario2_df['DLabel'].tolist())\n",
        "# Create PyTorch dataset for scenario 2\n",
        "train_dataset_scenario2 = TensorDataset(torch.tensor(train_encodings_scenario2['input_ids']),\n",
        "                                        torch.tensor(train_encodings_scenario2['attention_mask']),\n",
        "                                        train_labels_scenario2)\n",
        "# Define data loader for scenario 2\n",
        "train_dataloader_scenario2 = DataLoader(train_dataset_scenario2, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Load the pre-trained GPT2 model for sequence classification\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
        "model.resize_token_embeddings(len(tokenizer))  # Resize token embeddings to match the tokenizer's vocabulary size\n",
        "model.to(device)  # Move the model to the appropriate device\n",
        "\n",
        "# Ensure that the model's configuration has the pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader_scenario1))\n",
        "\n",
        "# Training loop for scenario 1\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader_scenario1:\n",
        "        input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Optional: gradient clipping\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader_scenario1)\n",
        "    print(f'Scenario 1 - Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "# Training loop for scenario 2\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader_scenario2:\n",
        "        input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Optional: gradient clipping\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader_scenario2)\n",
        "    print(f'Scenario 2 - Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "# Specify the directory path where the notebook file is located\n",
        "notebook_directory = '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "# Save the model to the notebook directory\n",
        "save_path = os.path.join(notebook_directory, \"Lastmodel.pth\")\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(\"Model saved successfully!\" if os.path.exists(save_path) else \"Failed to save the model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW0Ja7_XbvH3",
        "outputId": "194955af-86bf-4e42-d20b-fdfc17a86c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JEVXUblvt7gd0OIgT84SjTU30VyMi67V\n",
            "To: /content/scenario1.csv\n",
            "100%|██████████| 1.20M/1.20M [00:00<00:00, 35.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16g6vuDCkPUUGmjHWi9F9KDPaxuclW2ox\n",
            "To: /content/scenario2.csv\n",
            "100%|██████████| 1.38M/1.38M [00:00<00:00, 40.1MB/s]\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scenario 1 - Epoch 1/3, Training Loss: 0.7147\n",
            "Scenario 1 - Epoch 2/3, Training Loss: 0.6911\n",
            "Scenario 1 - Epoch 3/3, Training Loss: 0.6883\n",
            "Scenario 2 - Epoch 1/3, Training Loss: 0.6909\n",
            "Scenario 2 - Epoch 2/3, Training Loss: 0.6907\n",
            "Scenario 2 - Epoch 3/3, Training Loss: 0.6936\n",
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation"
      ],
      "metadata": {
        "id": "kz4zZDVap4J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2ForSequenceClassification, GPT2Config, AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_model(model_path, pretrained_tokenizer):\n",
        "    # Load the pretrained GPT2 model for sequence classification\n",
        "    model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
        "\n",
        "    # Load the trained model state dictionary\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')), strict=False)\n",
        "\n",
        "    # Investigate the error message\n",
        "    print(\"Investigating the error message:\")\n",
        "    print(\"Model Configuration:\")\n",
        "    print(model.config)\n",
        "\n",
        "    # Load the checkpoint configuration\n",
        "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "    print(\"Checkpoint Configuration:\")\n",
        "    print(checkpoint['config'].vocab_size)\n",
        "\n",
        "    # Compare vocabulary sizes\n",
        "    if checkpoint['config'].vocab_size != model.config.vocab_size:\n",
        "        print(\"Resizing the embedding layer...\")\n",
        "        # Resize the embedding layer\n",
        "        model.resize_token_embeddings(checkpoint['config'].vocab_size)\n",
        "        # Load the checkpoint again\n",
        "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')), strict=False)\n",
        "        print(\"Embedding layer resized and checkpoint loaded.\")\n",
        "\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_dataset_from_url(url):\n",
        "    # Load dataset from URL\n",
        "    response = requests.get(url)\n",
        "    df = pd.read_csv(io.StringIO(response.text))\n",
        "    return df\n",
        "\n",
        "def evaluate_model(model, test_data, scenario, tokenizer, batch_size=16):\n",
        "    if scenario == 1:\n",
        "        # Tokenize input text for scenario 1\n",
        "        test_encodings = tokenizer(test_data['Name1'].tolist(), test_data['Name2'].tolist(), truncation=True, padding=True)\n",
        "    elif scenario == 2:\n",
        "        # Tokenize input text for scenario 2\n",
        "        test_encodings = tokenizer(test_data['PName'].tolist(), test_data['DName'].tolist(), truncation=True, padding=True)\n",
        "\n",
        "    # Create PyTorch dataset for test data\n",
        "    test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
        "                                  torch.tensor(test_encodings['attention_mask']),\n",
        "                                  torch.tensor(test_data['DLabel'].tolist()))\n",
        "\n",
        "    # Define data loader for test data\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Lists to store true labels and predicted labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    # Iterate over test data batches\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Get predicted labels\n",
        "        predicted_class = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Append true and predicted labels\n",
        "        true_labels.extend(labels.tolist())\n",
        "        predicted_labels.extend(predicted_class.tolist())\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    report = classification_report(true_labels, predicted_labels, target_names=['No Interaction', 'Interaction'])\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "    # Print evaluation metrics\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "def main():\n",
        "    # Load the pretrained tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # Specify the path to your trained model\n",
        "    model_path = \"/content/drive/My Drive/Colab Notebooks/Lastmodel.pth\"\n",
        "\n",
        "    # Load datasets for evaluation\n",
        "    scenario1_test_url = \"*****\"\n",
        "    scenario2_test_url = \"*****\"\n",
        "\n",
        "    scenario1_test_data = load_dataset_from_url(scenario1_test_url)\n",
        "    scenario2_test_data = load_dataset_from_url(scenario2_test_url)\n",
        "\n",
        "    # Load your trained model\n",
        "    model = load_model(model_path, tokenizer)\n",
        "\n",
        "    # Evaluate the model for scenario 1\n",
        "    print(\"Evaluation for Scenario 1:\")\n",
        "    evaluate_model(model, scenario1_test_data, scenario=1, tokenizer=tokenizer)\n",
        "\n",
        "    # Evaluate the model for scenario 2\n",
        "    print(\"\\nEvaluation for Scenario 2:\")\n",
        "    evaluate_model(model, scenario2_test_data, scenario=2, tokenizer=tokenizer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "0Y_ObOCBYy33",
        "outputId": "04b7ebc0-d73e-430d-8486-fa5e93357c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for GPT2ForSequenceClassification:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6efbccb7e964>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-6efbccb7e964>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Load your trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Evaluate the model for scenario 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6efbccb7e964>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_path, pretrained_tokenizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Load the trained model state dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Investigate the error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2154\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT2ForSequenceClassification:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZcPBjHCouhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}